{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vn5wnnje505e"},"outputs":[],"source":["import sys\n","\n","!{sys.executable} -m pip install torch numpy transformers pandas"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"KZ6qK0ibJvoA","executionInfo":{"status":"ok","timestamp":1684683825588,"user_tz":-120,"elapsed":434,"user":{"displayName":"maxiior","userId":"11221372352955161135"}}},"outputs":[],"source":["import pandas as pd\n","\n","train_data = pd.read_csv(\"data/train.csv\")\n","\n","train_columns = ['qa_id', 'question_title', 'question_body', 'question_user_name',\n","       'question_user_page', 'answer', 'answer_user_name', 'answer_user_page',\n","       'url', 'category', 'host']\n","X = train_data[train_columns]\n","y = train_data.drop(train_columns, axis=1)\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1448,"status":"ok","timestamp":1684683827034,"user":{"displayName":"maxiior","userId":"11221372352955161135"},"user_tz":-120},"id":"aYHRvjRO6Pnm","outputId":"099f271c-b072-4ec6-a4bf-3801f9085489"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n"]}],"source":["import torch\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5618,"status":"ok","timestamp":1684683832651,"user":{"displayName":"maxiior","userId":"11221372352955161135"},"user_tz":-120},"id":"FxzgXDpEj181","outputId":"a67ddbcb-0e49-4234-f23d-ef937e53ce4a"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["from transformers import AutoTokenizer, AutoModel\n","import numpy as np\n","\n","MODEL = \"roberta-base\"\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","roberta = AutoModel.from_pretrained(MODEL).to(device)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"6syezvygCjFw","executionInfo":{"status":"ok","timestamp":1684683832651,"user_tz":-120,"elapsed":4,"user":{"displayName":"maxiior","userId":"11221372352955161135"}}},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader\n","\n","MAX_LENGTH_T = 30\n","MAX_LENGTH_Q = 128 # 128\n","MAX_LENGTH_A = 128 # 128\n","MAX_SEQUENCE = 160 # 290\n","\n","BATCH_SIZE = 40 # 40\n","\n","category_dict = {\n","  'LIFE_ARTS': 0,\n","  'STACKOVERFLOW': 1,\n","  'TECHNOLOGY': 2,\n","  'SCIENCE': 3,\n","  'CULTURE': 4\n","}\n","\n","def get_merged_text_plus_categories(X):\n","  T = X[\"question_title\"].tolist()\n","  Q = X[\"question_body\"].tolist()\n","  A = X[\"answer\"].tolist()\n","  C = X['category'].tolist()\n","  results = []\n","\n","  for t,q,a in zip(T,Q,A):\n","    merged = t.split()[:MAX_LENGTH_T] + [\"[SEP]\"] + q.split()[:MAX_LENGTH_Q] + [\"[SEP]\"] + a.split()[:MAX_LENGTH_A]\n","    results.append(\" \".join(i for i in merged))\n","\n","  X_tmp = [(i,j) for i,j in zip(results, [category_dict[x] for x in C])]\n","  X_train, X_valid, y_train, y_valid = train_test_split(X_tmp, y, test_size=0.2, random_state=42)\n","\n","  X_train_text = tokenizer([i[0] for i in X_train], truncation=True, padding='max_length', max_length=MAX_SEQUENCE, return_tensors='pt')\n","  X_valid_text = tokenizer([i[0] for i in X_valid], truncation=True, padding='max_length', max_length=MAX_SEQUENCE, return_tensors='pt')\n","\n","  X_train_category = torch.tensor([i[1] for i in X_train]).type(torch.float)\n","  X_valid_category = torch.tensor([i[1] for i in X_valid]).type(torch.float)\n","\n","  y_train = torch.tensor(y_train.values).type(torch.float)\n","  y_valid = torch.tensor(y_valid.values).type(torch.float)\n","\n","  ids = X_train_text['input_ids']\n","  attention_mask = X_train_text['attention_mask']\n","\n","  dataset_train = TensorDataset(ids, attention_mask, X_train_category, y_train)\n","  dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE)\n","\n","  ids = X_valid_text['input_ids']\n","  attention_mask = X_valid_text['attention_mask']\n","\n","  dataset_valid = TensorDataset(ids, attention_mask, X_valid_category, y_valid)\n","  dataloader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE)\n","\n","  return dataloader_train, dataloader_valid\n","\n","def get_merged_T_Q_plus_A_plus_categories(X):\n","  T = X[\"question_title\"].tolist()\n","  Q = X[\"question_body\"].tolist()\n","  A = X[\"answer\"].tolist()\n","  C = X['category'].tolist()\n","  results_T_Q = []\n","  results_A = []\n","\n","  for t,q in zip(T,Q):\n","    merged = t.split()[:MAX_LENGTH_T] + [\"[SEP]\"] + q.split()[:MAX_LENGTH_Q]\n","    results_T_Q.append(\" \".join(i for i in merged))\n","  \n","  for i in A:\n","    a = i.split()[:MAX_LENGTH_A]\n","    results_A.append(\" \".join(j for j in a))\n","\n","  X_tmp = [(i,j,k) for i,j,k in zip(results_T_Q, results_A, [category_dict[x] for x in C])]\n","  X_train, X_valid, y_train, y_valid = train_test_split(X_tmp, y, test_size=0.2, random_state=42)\n","\n","  X_train_T_Q = tokenizer([i[0] for i in X_train], truncation=True, padding='max_length', max_length=MAX_SEQUENCE, return_tensors='pt')\n","  X_valid_T_Q = tokenizer([i[0] for i in X_valid], truncation=True, padding='max_length', max_length=MAX_SEQUENCE, return_tensors='pt')\n","\n","  X_train_A = tokenizer([i[1] for i in X_train], truncation=True, padding='max_length', max_length=MAX_LENGTH_A, return_tensors='pt')\n","  X_valid_A = tokenizer([i[1] for i in X_valid], truncation=True, padding='max_length', max_length=MAX_LENGTH_A, return_tensors='pt')\n","\n","  X_train_category = torch.tensor([i[2] for i in X_train]).type(torch.float)\n","  X_valid_category = torch.tensor([i[2] for i in X_valid]).type(torch.float)\n","\n","  y_train = torch.tensor(y_train.values).type(torch.float)\n","  y_valid = torch.tensor(y_valid.values).type(torch.float)\n","\n","  ids_T_Q = X_train_T_Q['input_ids']\n","  attention_mask_T_Q = X_train_T_Q['attention_mask']\n","  ids_A = X_train_A['input_ids']\n","  attention_mask_A = X_train_A['attention_mask']\n","\n","  dataset_train = TensorDataset(ids_T_Q, attention_mask_T_Q, ids_A, attention_mask_A, X_train_category, y_train)\n","  dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE)\n","\n","  ids_T_Q = X_valid_T_Q['input_ids']\n","  attention_mask_T_Q = X_valid_T_Q['attention_mask']\n","  ids_A = X_valid_A['input_ids']\n","  attention_mask_A = X_valid_A['attention_mask']\n","\n","  dataset_valid = TensorDataset(ids_T_Q, attention_mask_T_Q, ids_A, attention_mask_A, X_valid_category, y_valid)\n","  dataloader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE)\n","\n","  return dataloader_train, dataloader_valid"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"DFZD1F7zCkvY","executionInfo":{"status":"ok","timestamp":1684683848141,"user_tz":-120,"elapsed":15493,"user":{"displayName":"maxiior","userId":"11221372352955161135"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# dataloader_train, dataloader_valid = get_merged_text_plus_categories(X)\n","\n","dataloader_train, dataloader_valid = get_merged_T_Q_plus_A_plus_categories(X)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Wi13Q6oqFEOB","executionInfo":{"status":"ok","timestamp":1684683848142,"user_tz":-120,"elapsed":52,"user":{"displayName":"maxiior","userId":"11221372352955161135"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","\n","class Model_Double_BERT(nn.Module):\n","    def __init__(self, bert, hidden_size=1024, output_size=30):\n","        super(Model_Double_BERT, self).__init__()\n","        self.bert = bert\n","        self.relu = nn.ReLU()\n","        self.fc1 = nn.Linear(1536, hidden_size)\n","        self.fc2 = nn.Linear(hidden_size, output_size)\n","        self.low_dropout = nn.Dropout(0.2)\n","        self.high_dropout = nn.Dropout(0.5)\n","\n","    def forward(self, input_ids_Q_T, mask_Q_T, input_ids_A, mask_A, category):\n","        x1 = self.bert(input_ids_Q_T, attention_mask=mask_Q_T)\n","        x1 = x1.pooler_output\n","        x2 = self.bert(input_ids_A, attention_mask=mask_A)\n","        x2 = x2.pooler_output\n","        x = torch.cat((x1, x2), dim=1)\n","        x = self.low_dropout(x)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.high_dropout(x)\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"fVEpbnsGnBFr","executionInfo":{"status":"ok","timestamp":1684683848143,"user_tz":-120,"elapsed":49,"user":{"displayName":"maxiior","userId":"11221372352955161135"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","\n","class Model(nn.Module):\n","    def __init__(self, bert, hidden_size=512, output_size=30):\n","        super(Model, self).__init__()\n","        self.bert = bert\n","        self.relu = nn.ReLU()\n","        self.fc1 = nn.Linear(768, hidden_size)\n","        self.fc2 = nn.Linear(hidden_size, output_size)\n","        self.low_dropout = nn.Dropout(0.2)\n","        self.high_dropout = nn.Dropout(0.5)\n","\n","    def forward(self, input_ids, mask, category):\n","        x = self.bert(input_ids, attention_mask=mask)\n","        x = x.pooler_output\n","        # x = torch.cat((x, category.unsqueeze(1)), dim=1)\n","        x = self.low_dropout(x)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.high_dropout(x)\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"mHvhH7x1smki","executionInfo":{"status":"ok","timestamp":1684683848144,"user_tz":-120,"elapsed":48,"user":{"displayName":"maxiior","userId":"11221372352955161135"}}},"outputs":[],"source":["def train(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    model.train()\n","    train_loss = 0\n","    for step, batch in enumerate(dataloader):\n","        batch = [i.to(device) for i in batch]\n","        sent_id_T_Q, mask_T_Q, sent_id_A, mask_A, category, y = batch\n","\n","        pred = model(sent_id_T_Q, mask_T_Q, sent_id_A, mask_A, category)\n","        loss = loss_fn(pred, y)\n","        train_loss += loss.item()\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        if step % 50 == 0:\n","            loss, current = loss.item(), step * len(sent_id_T_Q)\n","            print(f\"loss: {loss :>7f}  [{current:>5d}/{size:>5d}]\")\n","    train_loss /= num_batches\n","    print(f\"Avg train loss: {train_loss:>8f} \\n\")\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"KbCbcRH531-o","executionInfo":{"status":"ok","timestamp":1684683848145,"user_tz":-120,"elapsed":48,"user":{"displayName":"maxiior","userId":"11221372352955161135"}}},"outputs":[],"source":["from scipy import stats\n","import numpy as np\n","\n","def test(dataloader, model, loss_fn):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    model.eval()\n","    test_loss, correct = 0, 0\n","    preds = []\n","    labels = []\n","    sigmoid = torch.nn.Sigmoid()\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            batch = [i.to(device) for i in batch]\n","            sent_id_T_Q, mask_T_Q, sent_id_A, mask_A, category, y = batch\n","            pred = model(sent_id_T_Q, mask_T_Q, sent_id_A, mask_A, category)\n","            test_loss += loss_fn(pred, y).item()\n","            preds.append(pred)\n","            labels.append(y)\n","        test_loss /= num_batches\n","        print(f\"Avg test loss: {test_loss:>8f} \\n\")\n","        preds, labels = sigmoid(torch.cat(preds)), torch.cat(labels)\n","        spearman_coef = np.mean([stats.spearmanr(preds[:,i].cpu(),labels[:,i].cpu()).correlation for i in range(preds.shape[1])])\n","        print(f\"Avg spearman coef: {spearman_coef:>8f} \\n\")"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"n5ZkvpljVBoG","executionInfo":{"status":"ok","timestamp":1684683848145,"user_tz":-120,"elapsed":46,"user":{"displayName":"maxiior","userId":"11221372352955161135"}}},"outputs":[],"source":["model = Model_Double_BERT(roberta).to(device)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Q0MzXjEZ5VLE","executionInfo":{"status":"ok","timestamp":1684683848146,"user_tz":-120,"elapsed":46,"user":{"displayName":"maxiior","userId":"11221372352955161135"}}},"outputs":[],"source":["loss_fn = nn.BCEWithLogitsLoss().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JF1mZGWKxYJE","executionInfo":{"status":"ok","timestamp":1684691350412,"user_tz":-120,"elapsed":868091,"user":{"displayName":"maxiior","userId":"11221372352955161135"}},"outputId":"e0dcc208-90d7-4718-fcb2-c876f6955b07"},"outputs":[{"output_type":"stream","name":"stdout","text":["loss: 0.694176  [    0/ 4863]\n","loss: 0.509614  [ 2000/ 4863]\n","loss: 0.434722  [ 4000/ 4863]\n","Avg train loss: 0.516839 \n","\n","Avg test loss: 0.424644 \n","\n","Avg spearman coef: 0.149993 \n","\n","loss: 0.423653  [    0/ 4863]\n","loss: 0.433708  [ 2000/ 4863]\n","loss: 0.411865  [ 4000/ 4863]\n","Avg train loss: 0.423154 \n","\n","Avg test loss: 0.402801 \n","\n","Avg spearman coef: 0.230306 \n","\n","loss: 0.402474  [    0/ 4863]\n","loss: 0.417902  [ 2000/ 4863]\n","loss: 0.395803  [ 4000/ 4863]\n","Avg train loss: 0.407484 \n","\n","Avg test loss: 0.394826 \n","\n","Avg spearman coef: 0.280476 \n","\n","loss: 0.391383  [    0/ 4863]\n","loss: 0.409117  [ 2000/ 4863]\n","loss: 0.386159  [ 4000/ 4863]\n","Avg train loss: 0.398100 \n","\n","Avg test loss: 0.388806 \n","\n","Avg spearman coef: 0.303915 \n","\n","loss: 0.377464  [    0/ 4863]\n","loss: 0.396554  [ 2000/ 4863]\n","loss: 0.376496  [ 4000/ 4863]\n","Avg train loss: 0.389856 \n","\n","Avg test loss: 0.382841 \n","\n","Avg spearman coef: 0.322186 \n","\n","loss: 0.370707  [    0/ 4863]\n","loss: 0.396235  [ 2000/ 4863]\n","loss: 0.371710  [ 4000/ 4863]\n","Avg train loss: 0.382070 \n","\n","Avg test loss: 0.378618 \n","\n","Avg spearman coef: 0.332968 \n","\n","loss: 0.361966  [    0/ 4863]\n","loss: 0.382466  [ 2000/ 4863]\n","loss: 0.368932  [ 4000/ 4863]\n","Avg train loss: 0.376443 \n","\n","Avg test loss: 0.377133 \n","\n","Avg spearman coef: 0.338102 \n","\n","loss: 0.356897  [    0/ 4863]\n","loss: 0.373830  [ 2000/ 4863]\n","loss: 0.363996  [ 4000/ 4863]\n","Avg train loss: 0.370276 \n","\n","Avg test loss: 0.376043 \n","\n","Avg spearman coef: 0.343998 \n","\n","loss: 0.350385  [    0/ 4863]\n","loss: 0.368644  [ 2000/ 4863]\n","loss: 0.359441  [ 4000/ 4863]\n","Avg train loss: 0.365945 \n","\n","Avg test loss: 0.375952 \n","\n","Avg spearman coef: 0.346597 \n","\n","loss: 0.345669  [    0/ 4863]\n","loss: 0.365977  [ 2000/ 4863]\n","loss: 0.362763  [ 4000/ 4863]\n","Avg train loss: 0.362543 \n","\n","Avg test loss: 0.377475 \n","\n","Avg spearman coef: 0.349763 \n","\n","loss: 0.344209  [    0/ 4863]\n","loss: 0.362223  [ 2000/ 4863]\n","loss: 0.353228  [ 4000/ 4863]\n","Avg train loss: 0.359011 \n","\n","Avg test loss: 0.378802 \n","\n","Avg spearman coef: 0.347787 \n","\n","loss: 0.343866  [    0/ 4863]\n","loss: 0.356519  [ 2000/ 4863]\n","loss: 0.353696  [ 4000/ 4863]\n","Avg train loss: 0.356911 \n","\n","Avg test loss: 0.377950 \n","\n","Avg spearman coef: 0.349521 \n","\n","loss: 0.337246  [    0/ 4863]\n","loss: 0.352686  [ 2000/ 4863]\n","loss: 0.350421  [ 4000/ 4863]\n","Avg train loss: 0.353315 \n","\n","Avg test loss: 0.380601 \n","\n","Avg spearman coef: 0.345565 \n","\n","loss: 0.341331  [    0/ 4863]\n","loss: 0.350199  [ 2000/ 4863]\n","loss: 0.351989  [ 4000/ 4863]\n","Avg train loss: 0.350346 \n","\n","Avg test loss: 0.380781 \n","\n","Avg spearman coef: 0.346603 \n","\n","loss: 0.330866  [    0/ 4863]\n","loss: 0.344175  [ 2000/ 4863]\n","loss: 0.350790  [ 4000/ 4863]\n","Avg train loss: 0.348135 \n","\n","Avg test loss: 0.380137 \n","\n","Avg spearman coef: 0.350822 \n","\n","loss: 0.330084  [    0/ 4863]\n","loss: 0.346609  [ 2000/ 4863]\n","loss: 0.353598  [ 4000/ 4863]\n","Avg train loss: 0.346550 \n","\n","Avg test loss: 0.378388 \n","\n","Avg spearman coef: 0.351937 \n","\n","loss: 0.329154  [    0/ 4863]\n","loss: 0.339584  [ 2000/ 4863]\n","loss: 0.351839  [ 4000/ 4863]\n","Avg train loss: 0.344838 \n","\n","Avg test loss: 0.382060 \n","\n","Avg spearman coef: 0.355621 \n","\n","loss: 0.328153  [    0/ 4863]\n","loss: 0.342035  [ 2000/ 4863]\n","loss: 0.344656  [ 4000/ 4863]\n","Avg train loss: 0.342942 \n","\n","Avg test loss: 0.384750 \n","\n","Avg spearman coef: 0.353548 \n","\n","loss: 0.329241  [    0/ 4863]\n","loss: 0.341894  [ 2000/ 4863]\n","loss: 0.339169  [ 4000/ 4863]\n","Avg train loss: 0.338949 \n","\n","Avg test loss: 0.385255 \n","\n","Avg spearman coef: 0.354628 \n","\n","loss: 0.325263  [    0/ 4863]\n","loss: 0.332659  [ 2000/ 4863]\n","loss: 0.335959  [ 4000/ 4863]\n","Avg train loss: 0.336803 \n","\n","Avg test loss: 0.385700 \n","\n","Avg spearman coef: 0.354192 \n","\n","loss: 0.320600  [    0/ 4863]\n","loss: 0.329242  [ 2000/ 4863]\n","loss: 0.337056  [ 4000/ 4863]\n","Avg train loss: 0.333898 \n","\n","Avg test loss: 0.387447 \n","\n","Avg spearman coef: 0.352831 \n","\n","loss: 0.317655  [    0/ 4863]\n","loss: 0.327927  [ 2000/ 4863]\n","loss: 0.332541  [ 4000/ 4863]\n","Avg train loss: 0.332348 \n","\n","Avg test loss: 0.386103 \n","\n","Avg spearman coef: 0.352257 \n","\n","loss: 0.316005  [    0/ 4863]\n","loss: 0.326329  [ 2000/ 4863]\n","loss: 0.327450  [ 4000/ 4863]\n","Avg train loss: 0.330521 \n","\n","Avg test loss: 0.387988 \n","\n","Avg spearman coef: 0.352092 \n","\n","loss: 0.314947  [    0/ 4863]\n","loss: 0.326203  [ 2000/ 4863]\n","loss: 0.328786  [ 4000/ 4863]\n","Avg train loss: 0.329238 \n","\n","Avg test loss: 0.388119 \n","\n","Avg spearman coef: 0.350027 \n","\n","loss: 0.311409  [    0/ 4863]\n","loss: 0.320514  [ 2000/ 4863]\n","loss: 0.321977  [ 4000/ 4863]\n","Avg train loss: 0.326658 \n","\n","Avg test loss: 0.390514 \n","\n","Avg spearman coef: 0.348102 \n","\n","loss: 0.312115  [    0/ 4863]\n","loss: 0.323147  [ 2000/ 4863]\n","loss: 0.324553  [ 4000/ 4863]\n","Avg train loss: 0.324756 \n","\n","Avg test loss: 0.390057 \n","\n","Avg spearman coef: 0.350759 \n","\n","loss: 0.308504  [    0/ 4863]\n","loss: 0.322926  [ 2000/ 4863]\n","loss: 0.322560  [ 4000/ 4863]\n","Avg train loss: 0.322673 \n","\n","Avg test loss: 0.390819 \n","\n","Avg spearman coef: 0.349868 \n","\n","loss: 0.310392  [    0/ 4863]\n","loss: 0.318108  [ 2000/ 4863]\n","loss: 0.319010  [ 4000/ 4863]\n","Avg train loss: 0.320420 \n","\n","Avg test loss: 0.391445 \n","\n","Avg spearman coef: 0.350166 \n","\n","loss: 0.310425  [    0/ 4863]\n","loss: 0.319649  [ 2000/ 4863]\n","loss: 0.321970  [ 4000/ 4863]\n","Avg train loss: 0.319068 \n","\n","Avg test loss: 0.392324 \n","\n","Avg spearman coef: 0.350352 \n","\n","loss: 0.303288  [    0/ 4863]\n","loss: 0.315832  [ 2000/ 4863]\n","loss: 0.316725  [ 4000/ 4863]\n","Avg train loss: 0.318353 \n","\n","Avg test loss: 0.392485 \n","\n","Avg spearman coef: 0.349733 \n","\n","Done!\n"]}],"source":["epochs = 30\n","for t in range(epochs):\n","    train(dataloader_train, model, loss_fn, optimizer)\n","    test(dataloader_valid, model, loss_fn)\n","print(\"Done!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"veA97Zox7n0D"},"outputs":[],"source":["torch.save(model, \"model.pth\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"environment":{"kernel":"python3","name":"pytorch-gpu.1-13.m103","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m103"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":0}