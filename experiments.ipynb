{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vn5wnnje505e"},"outputs":[],"source":["import sys\n","\n","!{sys.executable} -m pip install torch numpy transformers pandas"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"KZ6qK0ibJvoA","executionInfo":{"status":"ok","timestamp":1683977856825,"user_tz":-120,"elapsed":1446,"user":{"displayName":"maxiior","userId":"11221372352955161135"}}},"outputs":[],"source":["import pandas as pd\n","\n","train_data = pd.read_csv(\"data/train.csv\")\n","\n","train_columns = ['qa_id', 'question_title', 'question_body', 'question_user_name',\n","       'question_user_page', 'answer', 'answer_user_name', 'answer_user_page',\n","       'url', 'category', 'host']\n","X = train_data[train_columns]\n","y = train_data.drop(train_columns, axis=1)\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aYHRvjRO6Pnm","outputId":"a83029dc-d0fb-40a2-b065-f5b89cd33253","executionInfo":{"status":"ok","timestamp":1683977861178,"user_tz":-120,"elapsed":3905,"user":{"displayName":"maxiior","userId":"11221372352955161135"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n"]}],"source":["import torch\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FxzgXDpEj181","outputId":"8b900f71-5a60-4992-f742-eae0c8ff367a","executionInfo":{"status":"ok","timestamp":1683977869779,"user_tz":-120,"elapsed":8606,"user":{"displayName":"maxiior","userId":"11221372352955161135"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["from transformers import AutoTokenizer, AutoModel\n","import numpy as np\n","\n","MODEL = \"roberta-base\"\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","roberta = AutoModel.from_pretrained(MODEL).to(device)"]},{"cell_type":"code","source":["MAX_LENGTH_T = 30\n","MAX_LENGTH_Q = 164 # 128\n","MAX_LENGTH_A = 164 # 128\n","MAX_SEQUENCE = 360 # 290\n","\n","BATCH_SIZE = 30 # 40\n","\n","category_dict = {\n","  'LIFE_ARTS': 0,\n","  'STACKOVERFLOW': 1,\n","  'TECHNOLOGY': 2,\n","  'SCIENCE': 3,\n","  'CULTURE': 4\n","}\n","\n","def get_merged(X):\n","  T = X[\"question_title\"].tolist()\n","  Q = X[\"question_body\"].tolist()\n","  A = X[\"answer\"].tolist()\n","  C = X['category'].tolist()\n","  results = []\n","\n","  for t,q,a in zip(T,Q,A):\n","    merged = t.split()[:MAX_LENGTH_T] + [\"[SEP]\"] + q.split()[:MAX_LENGTH_Q] + [\"[SEP]\"] + a.split()[:MAX_LENGTH_A]\n","    results.append(\" \".join(i for i in merged))\n","\n","  return [(i,j) for i,j in zip(results, [category_dict[i] for i in C])]"],"metadata":{"id":"6syezvygCjFw","executionInfo":{"status":"ok","timestamp":1683977869781,"user_tz":-120,"elapsed":16,"user":{"displayName":"maxiior","userId":"11221372352955161135"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_tmp = get_merged(X)\n","\n","X_train, X_valid, y_train, y_valid = train_test_split(X_tmp, y, test_size=0.2, random_state=42)\n","\n","X_train_text = tokenizer([i[0] for i in X_train], truncation=True, padding='max_length', max_length=MAX_SEQUENCE, return_tensors='pt')\n","X_valid_text = tokenizer([i[0] for i in X_valid], truncation=True, padding='max_length', max_length=MAX_SEQUENCE, return_tensors='pt')\n","\n","X_train_category = torch.tensor([i[1] for i in X_train]).type(torch.float)\n","X_valid_category = torch.tensor([i[1] for i in X_valid]).type(torch.float)\n","\n","y_train = torch.tensor(y_train.values).type(torch.float)\n","y_valid = torch.tensor(y_valid.values).type(torch.float)"],"metadata":{"id":"DFZD1F7zCkvY","executionInfo":{"status":"ok","timestamp":1683977881226,"user_tz":-120,"elapsed":11458,"user":{"displayName":"maxiior","userId":"11221372352955161135"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader\n","\n","ids = X_train_text['input_ids']\n","attention_mask = X_train_text['attention_mask']\n","\n","dataset_train = TensorDataset(ids, attention_mask, X_train_category, y_train)\n","dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE)\n","\n","ids = X_valid_text['input_ids']\n","attention_mask = X_valid_text['attention_mask']\n","\n","dataset_valid = TensorDataset(ids, attention_mask, X_valid_category, y_valid)\n","dataloader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE)"],"metadata":{"id":"Wi13Q6oqFEOB","executionInfo":{"status":"ok","timestamp":1683977881227,"user_tz":-120,"elapsed":6,"user":{"displayName":"maxiior","userId":"11221372352955161135"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"fVEpbnsGnBFr","executionInfo":{"status":"ok","timestamp":1683977881227,"user_tz":-120,"elapsed":5,"user":{"displayName":"maxiior","userId":"11221372352955161135"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","\n","class Model(nn.Module):\n","    def __init__(self, bert, hidden_size=512, output_size=30):\n","        super(Model, self).__init__()\n","        self.bert = bert\n","        self.relu = nn.ReLU()\n","        self.fc1 = nn.Linear(768, hidden_size)\n","        self.fc2 = nn.Linear(hidden_size, output_size)\n","        self.low_dropout = nn.Dropout(0.2)\n","        self.high_dropout = nn.Dropout(0.5)\n","\n","    def forward(self, input_ids, mask, category):\n","        x = self.bert(input_ids, attention_mask=mask)\n","        x = x.pooler_output\n","        # x = torch.cat((x, category.unsqueeze(1)), dim=1)\n","        x = self.low_dropout(x)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.high_dropout(x)\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"mHvhH7x1smki","executionInfo":{"status":"ok","timestamp":1683977881228,"user_tz":-120,"elapsed":5,"user":{"displayName":"maxiior","userId":"11221372352955161135"}}},"outputs":[],"source":["def train(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    model.train()\n","    train_loss = 0\n","    for step, batch in enumerate(dataloader):\n","        batch = [i.to(device) for i in batch]\n","        sent_id, mask, category, y = batch\n","\n","        pred = model(sent_id, mask, category)\n","        loss = loss_fn(pred, y)\n","        train_loss += loss.item()\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        if step % 50 == 0:\n","            loss, current = loss.item(), step * len(sent_id)\n","            print(f\"loss: {loss :>7f}  [{current:>5d}/{size:>5d}]\")\n","    train_loss /= num_batches\n","    print(f\"Avg train loss: {train_loss:>8f} \\n\")\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"KbCbcRH531-o","executionInfo":{"status":"ok","timestamp":1683977883169,"user_tz":-120,"elapsed":3,"user":{"displayName":"maxiior","userId":"11221372352955161135"}}},"outputs":[],"source":["from scipy import stats\n","import numpy as np\n","\n","def test(dataloader, model, loss_fn):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    model.eval()\n","    test_loss, correct = 0, 0\n","    preds = []\n","    labels = []\n","    sigmoid = torch.nn.Sigmoid()\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            batch = [i.to(device) for i in batch]\n","            sent_id, mask, category, y = batch\n","            pred = model(sent_id, mask, category)\n","            test_loss += loss_fn(pred, y).item()\n","            preds.append(pred)\n","            labels.append(y)\n","        test_loss /= num_batches\n","        print(f\"Avg test loss: {test_loss:>8f} \\n\")\n","        preds, labels = sigmoid(torch.cat(preds)), torch.cat(labels)\n","        spearman_coef = np.mean([stats.spearmanr(preds[:,i].cpu(),labels[:,i].cpu()).correlation for i in range(preds.shape[1])])\n","        print(f\"Avg spearman coef: {spearman_coef:>8f} \\n\")"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"n5ZkvpljVBoG","executionInfo":{"status":"ok","timestamp":1683977885511,"user_tz":-120,"elapsed":279,"user":{"displayName":"maxiior","userId":"11221372352955161135"}}},"outputs":[],"source":["model = Model(roberta).to(device)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Q0MzXjEZ5VLE","executionInfo":{"status":"ok","timestamp":1683977885723,"user_tz":-120,"elapsed":2,"user":{"displayName":"maxiior","userId":"11221372352955161135"}}},"outputs":[],"source":["loss_fn = nn.BCEWithLogitsLoss().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JF1mZGWKxYJE","outputId":"2c512ada-fc35-4fe1-c6d5-9d2a531c7411"},"outputs":[{"output_type":"stream","name":"stdout","text":["loss: 0.692894  [    0/ 4863]\n","loss: 0.574014  [ 1500/ 4863]\n","loss: 0.499494  [ 3000/ 4863]\n","loss: 0.471962  [ 4500/ 4863]\n","Avg train loss: 0.541078 \n","\n","Avg test loss: 0.442804 \n","\n","Avg spearman coef: 0.178605 \n","\n","loss: 0.449295  [    0/ 4863]\n","loss: 0.459741  [ 1500/ 4863]\n","loss: 0.422534  [ 3000/ 4863]\n","loss: 0.433848  [ 4500/ 4863]\n","Avg train loss: 0.442420 \n","\n","Avg test loss: 0.410376 \n","\n","Avg spearman coef: 0.214409 \n","\n","loss: 0.419541  [    0/ 4863]\n","loss: 0.431510  [ 1500/ 4863]\n","loss: 0.405355  [ 3000/ 4863]\n","loss: 0.417754  [ 4500/ 4863]\n","Avg train loss: 0.421438 \n","\n","Avg test loss: 0.397599 \n","\n","Avg spearman coef: 0.270298 \n","\n","loss: 0.399614  [    0/ 4863]\n","loss: 0.418656  [ 1500/ 4863]\n","loss: 0.388044  [ 3000/ 4863]\n","loss: 0.411545  [ 4500/ 4863]\n","Avg train loss: 0.409075 \n","\n","Avg test loss: 0.390607 \n","\n","Avg spearman coef: 0.285905 \n","\n","loss: 0.390416  [    0/ 4863]\n","loss: 0.406429  [ 1500/ 4863]\n","loss: 0.382873  [ 3000/ 4863]\n","loss: 0.403862  [ 4500/ 4863]\n","Avg train loss: 0.399421 \n","\n","Avg test loss: 0.385212 \n","\n","Avg spearman coef: 0.297357 \n","\n","loss: 0.377238  [    0/ 4863]\n","loss: 0.405081  [ 1500/ 4863]\n","loss: 0.374990  [ 3000/ 4863]\n","loss: 0.394868  [ 4500/ 4863]\n","Avg train loss: 0.391399 \n","\n","Avg test loss: 0.382858 \n","\n","Avg spearman coef: 0.304018 \n","\n","loss: 0.373007  [    0/ 4863]\n","loss: 0.393282  [ 1500/ 4863]\n","loss: 0.372930  [ 3000/ 4863]\n","loss: 0.390985  [ 4500/ 4863]\n","Avg train loss: 0.385042 \n","\n","Avg test loss: 0.380276 \n","\n","Avg spearman coef: 0.312546 \n","\n","loss: 0.370073  [    0/ 4863]\n","loss: 0.393361  [ 1500/ 4863]\n","loss: 0.362975  [ 3000/ 4863]\n","loss: 0.380204  [ 4500/ 4863]\n","Avg train loss: 0.379815 \n","\n","Avg test loss: 0.377937 \n","\n","Avg spearman coef: 0.323957 \n","\n","loss: 0.364036  [    0/ 4863]\n","loss: 0.390566  [ 1500/ 4863]\n","loss: 0.358171  [ 3000/ 4863]\n","loss: 0.376911  [ 4500/ 4863]\n","Avg train loss: 0.375434 \n","\n","Avg test loss: 0.379813 \n","\n","Avg spearman coef: 0.323917 \n","\n","loss: 0.360519  [    0/ 4863]\n","loss: 0.390081  [ 1500/ 4863]\n","loss: 0.353674  [ 3000/ 4863]\n","loss: 0.369202  [ 4500/ 4863]\n","Avg train loss: 0.371517 \n","\n","Avg test loss: 0.380272 \n","\n","Avg spearman coef: 0.327942 \n","\n","loss: 0.354318  [    0/ 4863]\n","loss: 0.385622  [ 1500/ 4863]\n","loss: 0.353810  [ 3000/ 4863]\n","loss: 0.369367  [ 4500/ 4863]\n","Avg train loss: 0.368544 \n","\n","Avg test loss: 0.378838 \n","\n","Avg spearman coef: 0.331503 \n","\n","loss: 0.352170  [    0/ 4863]\n","loss: 0.385785  [ 1500/ 4863]\n","loss: 0.354950  [ 3000/ 4863]\n","loss: 0.352549  [ 4500/ 4863]\n","Avg train loss: 0.364987 \n","\n","Avg test loss: 0.378990 \n","\n","Avg spearman coef: 0.332443 \n","\n","loss: 0.351013  [    0/ 4863]\n","loss: 0.378338  [ 1500/ 4863]\n","loss: 0.347422  [ 3000/ 4863]\n","loss: 0.356790  [ 4500/ 4863]\n","Avg train loss: 0.361427 \n","\n","Avg test loss: 0.380021 \n","\n","Avg spearman coef: 0.327637 \n","\n","loss: 0.347758  [    0/ 4863]\n","loss: 0.368003  [ 1500/ 4863]\n","loss: 0.351893  [ 3000/ 4863]\n","loss: 0.349491  [ 4500/ 4863]\n","Avg train loss: 0.358324 \n","\n","Avg test loss: 0.380837 \n","\n","Avg spearman coef: 0.335496 \n","\n","loss: 0.341024  [    0/ 4863]\n","loss: 0.370873  [ 1500/ 4863]\n","loss: 0.345515  [ 3000/ 4863]\n","loss: 0.349973  [ 4500/ 4863]\n","Avg train loss: 0.356096 \n","\n","Avg test loss: 0.382276 \n","\n","Avg spearman coef: 0.335181 \n","\n","loss: 0.341320  [    0/ 4863]\n","loss: 0.361542  [ 1500/ 4863]\n","loss: 0.332270  [ 3000/ 4863]\n","loss: 0.342366  [ 4500/ 4863]\n","Avg train loss: 0.353440 \n","\n","Avg test loss: 0.383249 \n","\n","Avg spearman coef: 0.330842 \n","\n","loss: 0.337930  [    0/ 4863]\n","loss: 0.358920  [ 1500/ 4863]\n","loss: 0.336993  [ 3000/ 4863]\n","loss: 0.348848  [ 4500/ 4863]\n","Avg train loss: 0.350051 \n","\n","Avg test loss: 0.384342 \n","\n","Avg spearman coef: 0.328514 \n","\n","loss: 0.342249  [    0/ 4863]\n","loss: 0.358838  [ 1500/ 4863]\n","loss: 0.329252  [ 3000/ 4863]\n","loss: 0.340235  [ 4500/ 4863]\n","Avg train loss: 0.347580 \n","\n","Avg test loss: 0.384601 \n","\n","Avg spearman coef: 0.331889 \n","\n","loss: 0.336147  [    0/ 4863]\n","loss: 0.353955  [ 1500/ 4863]\n","loss: 0.328759  [ 3000/ 4863]\n","loss: 0.335882  [ 4500/ 4863]\n","Avg train loss: 0.344948 \n","\n","Avg test loss: 0.384820 \n","\n","Avg spearman coef: 0.333440 \n","\n","loss: 0.328320  [    0/ 4863]\n","loss: 0.345064  [ 1500/ 4863]\n","loss: 0.331162  [ 3000/ 4863]\n","loss: 0.332726  [ 4500/ 4863]\n","Avg train loss: 0.342897 \n","\n","Avg test loss: 0.386052 \n","\n","Avg spearman coef: 0.331478 \n","\n","loss: 0.330162  [    0/ 4863]\n","loss: 0.348329  [ 1500/ 4863]\n","loss: 0.330515  [ 3000/ 4863]\n","loss: 0.327551  [ 4500/ 4863]\n","Avg train loss: 0.340645 \n","\n","Avg test loss: 0.387736 \n","\n","Avg spearman coef: 0.329940 \n","\n","loss: 0.331406  [    0/ 4863]\n","loss: 0.344786  [ 1500/ 4863]\n","loss: 0.328943  [ 3000/ 4863]\n","loss: 0.333358  [ 4500/ 4863]\n","Avg train loss: 0.338578 \n","\n","Avg test loss: 0.389653 \n","\n","Avg spearman coef: 0.333298 \n","\n","loss: 0.327071  [    0/ 4863]\n","loss: 0.343363  [ 1500/ 4863]\n","loss: 0.319172  [ 3000/ 4863]\n","loss: 0.326534  [ 4500/ 4863]\n","Avg train loss: 0.336302 \n","\n","Avg test loss: 0.390192 \n","\n","Avg spearman coef: 0.331246 \n","\n","loss: 0.323745  [    0/ 4863]\n","loss: 0.339221  [ 1500/ 4863]\n","loss: 0.322490  [ 3000/ 4863]\n"]}],"source":["epochs = 30\n","for t in range(epochs):\n","    train(dataloader_train, model, loss_fn, optimizer)\n","    test(dataloader_valid, model, loss_fn)\n","print(\"Done!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"veA97Zox7n0D"},"outputs":[],"source":["torch.save(model, \"model.pth\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"environment":{"kernel":"python3","name":"pytorch-gpu.1-13.m103","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m103"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":0}